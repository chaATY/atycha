---
title: 深度学习框架 & 推理引擎
date: 2023-10-30
author: CHA.ATY
tags:
  - AI
---

# 一、简介

简单来说，对于机器学习模型过程可分为**训练迭代**和**部署上线**两个方面：

- **训练迭代**，即通过特定的数据集、模型结构、损失函数和评价指标的确定，到模型参数的训练，以尽可能达到SOTA(State of the Art)的结果。
- **部署上线**，即指让训练好的模型在特定环境中运行的过程，更多关注于部署场景、部署方式、吞吐率和延迟。

在实际场景中，深度学习模型通常通过PyTorch、TensorFlow等框架来完成，直接通过这些模型来进行推理效率并不高，特别是对延时要求严格的线上场景。由此，经过工业界和学术界数年的探索，模型部署有了一条流行的流水线：
- 训练，深度学习框架：Pytorch、TensorFlow、Caffe、mxnet
- 优化，中间表示：ONNX、Pytorch、Caffe
- 运行，推理引擎：TensorRT、ONNX Runtime、NCNN、OpenVINO、Mediapipe

---

# 二、深度学习框架

---

# 三、推理引擎

|模型推理部署框架|应用平台|
|---|---|
|NCNN|移动端|
|OpenVINO|CPU，GPU，嵌入式平台都可以使用，尤其是在CPU上首选OPenVINO。DepthAI嵌入式空间AI平台。|
|TensorRT|只能用在NIVDIA的GPU上的推理框架。NIVDIA的Jetson平台。|
|Mediapipe|服务端，移动端，嵌入式平台，TPU。|

研发单位：
- 腾讯公司开发的移动端平台部署工具——NCNN；
- Intel公司针对自家设备开开发的部署工具——OpenVINO；
- NVIDIA公司针对自家GPU开发的部署工具——TensorRT；
- Google针对自家硬件设备和深度学习框架开发的部署工具——Mediapipe；
- 由微软、亚马逊 、Facebook 和 IBM 等公司共同开发的开放神经网络交换格式——ONNX；

如何选择：
- ONNXRuntime 
	- ONNXRuntime支持多种显卡加速，包括但不限于以下几种：
		- NVIDIA GPU：使用CUDA加速
		- AMD GPU：使用ROCm加速
		- Intel GPU：使用OpenVINO加速
	- 可以运行在多平台 (Windows，Linux，Mac，Android，iOS) 上的一款推理框架，它接受 ONNX 格式的模型输入，支持 GPU 和 CPU 的推理。唯一不足就是 ONNX 节点粒度较细，推理速度有时候比其他推理框架如 TensorRT 较低。
- NCNN是针对手机端的部署。优势是开源较早，有非常稳定的社区，开源影响力也较高。
- OpenVINO 是 Intel 家出的针对 Intel 出品的 CPU 和 GPU 友好的一款推理框架，同时它也是对接不同训练框架如 TensorFlow，Pytorch，Caffe 等。不足之处可能是只支持 Intel 家的硬件产品。
- TensorRT 针对 NVIDIA 系列显卡具有其他框架都不具备的优势，如果运行在 NVIDIA 显卡上， TensorRT 一般是所有框架中推理最快的。一般的主流的训练框架如TensorFlow 和 Pytorch 都能转换成 TensorRT 可运行的模型。当然了，TensorRT 的限制就是只能运行在 NVIDIA 显卡上，同时不开源 kernel。
- MediaPipe 不支持除了tensorflow之外的其他深度学习框架。MediaPipe 的主要用例是使用推理模型和其他可重用组件对应用机器学习管道进行快速原型设计。MediaPipe 还有助于将机器学习技术部署到各种不同硬件平台上的演示和应用程序中，为移动、桌面/云、web和物联网设备构建世界级ML解决方案和应用程序。
- CPU 上速度最快的是 OpenVINO，GPU 上速度最快的是 TensorRT，能不改代码同时支持CPU跟GPU推理的是 ONNXRUNTIME，OpenCV DNN毫无意外的速度最慢（CPU/GPU）

[Pytorch模型转onnx打包部署(完全脱离pytorch)](https://blog.csdn.net/qq_44932092/article/details/127756016#t12)