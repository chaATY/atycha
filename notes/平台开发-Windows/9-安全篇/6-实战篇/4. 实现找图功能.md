---
title: 找图功能
date: 2023-11-02 19:57
author: CHA.ATY
tags:
  - Python
  - Opencv
  - 功能实现
category: 功能实现
---

![](https://img.shields.io/badge/opencv_python-4.8.1.78-green.svg) [![](https://img.shields.io/badge/前置知识-opencv-orange.svg)](/笔记-AI/_sidebar.md)

# 一、简介

使用opencv实现大图找小图，主要是用2张图片进行特征对比，根据对比结果找到小图在大图中的坐标等信息。

# 二、为什么？

在Game脚本中这是一个核心功能，应用场景较广，使用频率高。

# 三、实现思路

首先使用Opencv实现找图有以下几点方案：

## 1. 模板匹配

[模板匹配](https://blog.csdn.net/qq_45832961/article/details/122455118)就是在整个图像区域发现与给定子图像匹配的小块区域。

模板匹配具有自身的局限性，主要表现在它只能进行平行移动，若原图像中的匹配目标发生旋转或大小变化，该算法无效。

**工作原理：**在待检测图像上，从左到右，从上向下计算模板图像与重叠子图像的匹配度，匹配程度越大，两者相同的可能性越大。
1. 模板匹配即用大小为 W*H 的模板在大小为 Q*S 的图像上滑动，并使用参数method中指定的方法，将模板与图像的所有重叠部分进行比较，然后将比较结果存储在结果 res 中。
显然，图像 (Q*S) 必须大于模板 (W*H) ，且最终的结果 res 的大小为（Q-W+1)*(S-H+1)。
2. 如果匹配方法中含有_NORMED，那么模板匹配函数之后就不再需要进行归一化处理，因为结果将在0和1的范围内，否则，需要使用 normalize 函数对结果进行归一化处理。
3. 完成匹配后，使用cv.minMaxLoc()方法定位结果图像的全局最小值（图像中的最暗点）和全局最大值（图像中的最亮点）。如果使用的是平方差匹配，则最小值位置是最佳匹配位置，否则，则最大值位置是最佳匹配位置。

**注意：**模板匹配不适用于尺度变换，视角变换后的图像，这时我们就要使用关键点匹配算法，比较经典的关键点检测算法包括SIFT和SURF等，主要的思路是首先通过关键点检测算法获取模板和测试图片中的关键点﹔然后使用关键点匹配算法处理即可，这些关键点可以很好的处理尺度变化、视角变换、旋转变化、光照变化等，具有很好的不变性。

**具体使用：**

```
import cv2 as cv
import numpy as np
 
def template_demo():
  tpl = cv.imread("./temp.png")
  target = cv.imread("./1.png")
  cv.imshow("template image",tpl)
  cv.imshow("target image",target)
  methods = [cv.TM_SQDIFF_NORMED,cv.TM_CCORR_NORMED,cv.TM_CCOEFF_NORMED]　　#各种匹配算法
  th,tw = tpl.shape[:2]　　#获取模板图像的高宽
  for md in methods:
    result = cv.matchTemplate(target,tpl,md)
    # result是我们各种算法下匹配后的图像
    # cv.imshow("%s"%md,result)
    #获取的是每种公式中计算出来的值，每个像素点都对应一个值
    min_val,max_val,min_loc,max_loc = cv.minMaxLoc(result)
    if md == cv.TM_SQDIFF_NORMED:
      tl = min_loc  #tl是左上角点
    else:
      tl = max_loc
    br = (tl[0]+tw,tl[1]+th)  #右下点
    cv.rectangle(target,tl,br,(0,0,255),2)　　#画矩形
    cv.imshow("match-%s"%md,target)
 
src = cv.imread("./1.png") #读取图片
cv.namedWindow("input image",cv.WINDOW_AUTOSIZE)  #创建GUI窗口,形式为自适应
cv.imshow("input image",src)  #通过名字将图像和窗口联系
template_demo()
cv.waitKey(0)  #等待用户操作，里面等待参数是毫秒，我们填写0，代表是永远，等待用户操作
cv.destroyAllWindows() #销毁所有窗口
```

## 2. 特征点找图

[特征点的检测算法](https://www.cnblogs.com/multhree/p/11296945.html)：
1. [ORB](https://blog.csdn.net/qq_45832961/article/details/122769960) (Oriented FAST and Rotated BRIEF) 发布于2011年，作为SIFT和SURF的一个快速替代品，是一种基于 FAST 角点检测和 BRIEF 描述子的特征检测算法。ORB 具有 SIFT 和 SURF 类似的尺度不变性和旋转不变性，但它的速度比 SIFT 和 SURF 更快。
2. [SIFT](https://blog.csdn.net/qq_45832961/article/details/122776322) (Scale-Invariant Feature Transform) 是一种基于尺度不变性的特征检测算法，它可以在图像中检测出具有不同尺度和方向的关键点。SIFT 特征具有很高的鲁棒性和匹配精度，在图像拼接、视觉定位等应用中有很好的表现。
3. SURFT (Speeded-Up Robust Features) 是一种基于尺度不变性的快速特征检测算法。SURF 使用了和 SIFT 类似的描述子，但采用了更快的检测方法，所以它的速度比 SIFT 快得多。在日常应用中，有SURF基本就不用考虑SIFT，SURF基本就是SIFT的全面升级版，当然也有其他SIFT的改进版比如Affine SIFT的效果就要比SUFR要好更多，但是计算时间也有延长，而ORB的强点在于计算时间。ORB主要还是在VSLAM中应用较多，场景变化不明显，但是需要高速的计算时间，这正好符合ORB。
4. Harris角点

**特侦点匹配方法：**BruteForce（暴力法）、FLANN、以及随机抽样一致性优化RANSAC算法

**比较总结：**
1. 计算速度: ORB>>SURF>>SIFT（各差一个量级，ORB较快，SURF运行速度大约为SIFT的3倍，ORB是sift的100倍，是surf的10倍。）
2. 旋转鲁棒性：SURF>ORB~SIFT（表示差不多）
3. 模糊鲁棒性：SURF>ORB~SIFT
4. 尺度变换鲁棒性: SURF>SIFT>ORB（ORB并不具备尺度变换性）
5. 时效性：FLANN>暴力求解

**注意：**Opencv4没有SURF算法，因此建议使用SIFT + FLANN实现找图功能

# 四、具体实现

```python
import cv2
import numpy as np
import time

start_time = time.time()

# 读取图片并转为灰度图
img1 = cv2.imread("./1.png", cv2.IMREAD_GRAYSCALE)
img2 = cv2.imread("./2.bmp", cv2.IMREAD_GRAYSCALE)

# 创建SIFT对象(特征检测器)，并计算灰度图像(描述符)，最大特征点数,需要修改，5000太大。
sift = cv2.SIFT_create()
# 检测关键点，生成描述符。kp,des = orb.detectAndCompute(img, None) img:原图；kp: 检测出的关键点；des:关键点描述符
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)

# 设置FLANN匹配器参数
indexParams = dict(algorithm=1, trees=5)
searchParams = dict(checks=50)
# FLANN匹配器
flann = cv2.FlannBasedMatcher(indexParams, searchParams)
# K-最近邻匹配
matches = flann.knnMatch(des1, des2, k=2)

print("耗时: {:.2f}秒".format(time.time() - start_time))

# 绘制图像
good_matches = []
for m, n in matches:
    if m.distance < 0.7 * n.distance:
        good_matches.append(m)

# 基于FLANN进行单应性匹配，把结果简化为更简洁的几何表示——单应性，他将描述整个匹配对象的姿态，而不是一堆不连续的点。
# 绘制一个空白图片
mask_matches = []
MIN_NUM_GOOD_MATCHES = 10
# 检测是否匹配
if len(good_matches) >= MIN_NUM_GOOD_MATCHES:
    src_pts = np.float32(
        [kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32(
        [kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
    mask_matches = mask.ravel().tolist()
 
    h, w = img1.shape[:2]
    src_corners = np.float32(
        [[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
    dst_corners = cv2.perspectiveTransform(src_corners, M)
    img2 = cv2.polylines(img2, [np.int32(dst_corners)], True, (255, 0, 0), 3, cv2.LINE_AA)
    print('It is a match!')
else:
    print('It is not a match!')
    mask_matches = None

# 图像参数
#drawParams = dict(matchColor=(0, 255, 0),
#                  singlePointColor=(255, 0, 0),
#                  matchesMask=matchesMask,
#                  flags=0)
drawParams = dict(matchColor=(0, 255, 0),
                  singlePointColor=None,
                  matchesMask=mask_matches,
                  flags=2)
# 获取最终结果image
#result_image = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_matches, None, **drawParams)
result_image = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, **drawParams)

# 设置显示窗口
cv2.namedWindow('img', 0)
cv2.resizeWindow('img', 840, 480)
cv2.imshow('img', result_image)
while True:
    if cv2.waitKey(0) & 0xff == ord('q'):
        break
cv2.destroyAllWindows()
```

![效果图1](找图-实现1.png)